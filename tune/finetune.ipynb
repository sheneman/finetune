{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMp4RKNtvtckh667QDn5q3o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheneman/finetune/blob/main/tune/finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1TiKZBObcNh"
      },
      "outputs": [],
      "source": [
        "# finetune.py\n",
        "#\n",
        "# Fine-tune GPT2 on library transcripts - prompt/response\n",
        "#\n",
        "# Luke Sheneman\n",
        "# Research Computing and Data Services (RCDS)\n",
        "# Institute for Interdisciplinary Data Sciences (IIDS)\n",
        "#\n",
        "# sheneman@uidaho.edu\n",
        "# January, 2024\n",
        "#\n",
        "\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "import json\n",
        "\n",
        "\n",
        "# some Hyperparameters\n",
        "BATCH_SIZE     = 12\n",
        "NUM_EPOCHS     = 3\n",
        "LEARNING_RATE  = 5e-5\n",
        "INPUT_FILE     = \"../data/prompt_response.json\"\n",
        "\n",
        "\n",
        "\n",
        "# Load GPT-2 model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "model      = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer  = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load and process data\n",
        "with open(INPUT_FILE, 'r') as file:\n",
        "\tdata = json.load(file)\n",
        "\n",
        "train_data = []\n",
        "for item in data:\n",
        "\tinput_text = \"Q: \" + item['PROMPT'] + \" A:\"\n",
        "\ttarget_text = item['RESPONSE']\n",
        "\ttrain_data.append((input_text, target_text))\n",
        "\n",
        "# Define a custom dataset\n",
        "class QADataset(Dataset):\n",
        "\tdef __init__(self, data, tokenizer):\n",
        "\t\tself.data = data\n",
        "\t\tself.tokenizer = tokenizer\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn len(self.data)\n",
        "\n",
        "\tdef __getitem__(self, idx):\n",
        "\t\tinput_text, target_text = self.data[idx]\n",
        "\t\tencoding = self.tokenizer(input_text, target_text, return_tensors='pt', max_length=512, padding='max_length', truncation=True)\n",
        "\t\treturn {key: val.squeeze() for key, val in encoding.items()}\n",
        "\n",
        "dataset = QADataset(train_data, tokenizer)\n",
        "loader  = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "number_of_batches = len(loader)\n",
        "\n",
        "# Set up training\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "model.train()  # set model to \"train\" mode (PyTorch)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "\tbatch_idx = 0\n",
        "\tfor batch in loader:\n",
        "\n",
        "\t\toptimizer.zero_grad()\n",
        "\n",
        "\t\tinput_ids = batch['input_ids'].to(device)\n",
        "\t\tattention_mask = batch['attention_mask'].to(device)\n",
        "\t\tlabels = batch['input_ids'].to(device)  # Labels are input ids\n",
        "\n",
        "\t\toutputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "\t\tloss = outputs.loss\n",
        "\t\tloss.backward()\n",
        "\t\toptimizer.step()\n",
        "\n",
        "\t\t# Report loss\n",
        "\t\tif(batch_idx % 100 == 0):\n",
        "\t\t\tprint(f\"Batch {batch_idx + 1}/{number_of_batches}, Loss: {loss.item()}\")\n",
        "\n",
        "\t\tbatch_idx += 1\n",
        "\n",
        "\tprint(f\"Epoch {epoch + 1} completed\")\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained('fine_tuned_gpt2_qa_model')\n",
        "tokenizer.save_pretrained('fine_tuned_gpt2_qa_model')\n"
      ]
    }
  ]
}